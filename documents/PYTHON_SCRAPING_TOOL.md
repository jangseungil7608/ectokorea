# Python ë©€í‹° ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ íˆ´

## ê°œìš”

ì¼ë³¸ ì£¼ìš” ì‡¼í•‘ëª°(Amazon, ë¼ì¿ í…, JINS)ì—ì„œ ìƒí’ˆ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ëŠ” í†µí•© Python ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. FastAPI ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ìˆìœ¼ë©° í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ì‹œìŠ¤í…œ êµ¬ì¡°

### ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```
python-scraper/
â”œâ”€â”€ main.py                    # FastAPI ì„œë²„ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ scraper.py         # API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ base_scraper.py    # ìŠ¤í¬ë˜í¼ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â”œâ”€â”€ scraper_factory.py # íŒ©í† ë¦¬ íŒ¨í„´ êµ¬í˜„
â”‚   â”‚   â””â”€â”€ exceptions.py      # ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì •ì˜
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ product.py         # í†µí•© ìƒí’ˆ ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ amazon/
â”‚   â”‚   â”‚   â””â”€â”€ amazon_scraper.py  # Amazon ìŠ¤í¬ë˜í¼ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ rakuten/           # ë¼ì¿ í… ìŠ¤í¬ë˜í¼ (TODO)
â”‚   â”‚   â””â”€â”€ jins/              # JINS ìŠ¤í¬ë˜í¼ (TODO)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ smart_extractor.py # AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ
â””â”€â”€ requirements.txt
```

### ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

## 1. API ì—”ë“œí¬ì¸íŠ¸

### ê¸°ë³¸ ì ‘ê·¼ URL
- **ë¡œì»¬**: `http://192.168.1.13:8001/ectokorea/api/v1`
- **ì™¸ë¶€**: `https://devseungil.mydns.jp:8001/ectokorea/api/v1`

### ì§€ì› API

#### 1) URL ìë™ ê°ì§€ ìŠ¤í¬ë˜í•‘
```bash
GET /ectokorea/api/v1/scrape?url={product_url}
```
**ì„¤ëª…**: URLì„ ë„˜ê²¨ì£¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ì´íŠ¸ë¥¼ ê°ì§€í•˜ê³  ì ì ˆí•œ ìŠ¤í¬ë˜í¼ë¡œ ì²˜ë¦¬

**ì˜ˆì‹œ**:
```bash
curl "http://192.168.1.13:8001/ectokorea/api/v1/scrape?url=https://www.amazon.co.jp/dp/B0DJNXJTJL"
```

#### 2) ì‚¬ì´íŠ¸ë³„ ì§ì ‘ ìŠ¤í¬ë˜í•‘
```bash
GET /ectokorea/api/v1/scrape/{site}
```

**Amazon**:
```bash
GET /ectokorea/api/v1/scrape/amazon?asin=B0DJNXJTJL
```

**ë¼ì¿ í…** (êµ¬í˜„ ì˜ˆì •):
```bash
GET /ectokorea/api/v1/scrape/rakuten?shopId={shopId}&itemCode={itemCode}
```

**JINS** (êµ¬í˜„ ì˜ˆì •):
```bash
GET /ectokorea/api/v1/scrape/jins?productId={productId}
```

#### 3) ì§€ì› ì‚¬ì´íŠ¸ ëª©ë¡
```bash
GET /ectokorea/api/v1/sites
```

## 2. íŒ©í† ë¦¬ íŒ¨í„´ êµ¬í˜„

### ScraperFactory í´ë˜ìŠ¤

**íŒŒì¼**: `app/core/scraper_factory.py`

#### ì£¼ìš” ê¸°ëŠ¥:
1. **URL íŒ¨í„´ ìë™ ê°ì§€**
```python
SITE_PATTERNS = {
    'amazon': {
        'regex': r'amazon\.co\.jp/dp/([A-Z0-9]{10})',
        'params': ['asin'],
        'url_template': 'https://www.amazon.co.jp/dp/{asin}'
    },
    'rakuten': {
        'regex': r'item\.rakuten\.co\.jp/([^/]+)/([^/?]+)',
        'params': ['shopId', 'itemCode'],
        'url_template': 'https://item.rakuten.co.jp/{shopId}/{itemCode}'
    },
    'jins': {
        'regex': r'jins\.com/jp/item/([^.]+)\.html',
        'params': ['productId'],
        'url_template': 'https://www.jins.com/jp/item/{productId}.html'
    }
}
```

2. **ë™ì  ìŠ¤í¬ë˜í¼ ìƒì„±**
```python
@classmethod
def create_scraper(cls, site: str) -> BaseScraper:
    if site == 'amazon':
        from app.scrapers.amazon.amazon_scraper import AmazonScraper
        return AmazonScraper()
    # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë“¤...
```

3. **URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ**
```python
@classmethod
def detect_site_from_url(cls, url: str) -> Tuple[str, Dict[str, str]]:
    for site, config in cls.SITE_PATTERNS.items():
        match = re.search(config['regex'], url)
        if match:
            params = {}
            for i, param in enumerate(config['params']):
                params[param] = match.group(i + 1)
            return site, params
```

## 3. í†µí•© ë°ì´í„° ëª¨ë¸

### Product í´ë˜ìŠ¤

**íŒŒì¼**: `app/models/product.py`

#### ë°ì´í„° êµ¬ì¡°:
```python
class Product(BaseModel):
    # ê¸°ë³¸ ì •ë³´
    site: str                    # ì‚¬ì´íŠ¸ëª…
    product_id: str              # ìƒí’ˆ ID
    url: str                     # ìƒí’ˆ URL
    
    # ìƒí’ˆ ìƒì„¸
    name: str                    # ìƒí’ˆëª…
    price: Optional[float]       # ê°€ê²© (JPY)
    currency: str = "JPY"        # í†µí™”
    
    # ì´ë¯¸ì§€ ë° ë¯¸ë””ì–´
    image_url: Optional[str]     # ë©”ì¸ ì´ë¯¸ì§€
    image_urls: List[str] = []   # ì„¤ëª… ì˜ì—­ ì´ë¯¸ì§€
    thumbnail_images: List[str] = []  # ì¸ë„¤ì¼ ê°¤ëŸ¬ë¦¬
    large_images: List[str] = []      # í° ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬
    
    # ìƒì„¸ ì •ë³´
    description: Optional[str]   # HTML í¬í•¨ ìƒí’ˆ ì„¤ëª…
    features: List[str] = []     # ì£¼ìš” íŠ¹ì§•
    specifications: Dict[str, Any] = {}  # ì‚¬ì–‘
    
    # ë¬¼ë¦¬ì  ì •ë³´
    weight: Optional[str]        # ë¬´ê²Œ
    dimensions: Optional[str]    # ì¹˜ìˆ˜
    
    # ì¹´í…Œê³ ë¦¬ ë° í‰ì 
    category: Optional[str]      # ì¹´í…Œê³ ë¦¬
    brand: Optional[str]         # ë¸Œëœë“œ
    rating: Optional[float]      # í‰ì 
    review_count: Optional[int]  # ë¦¬ë·° ìˆ˜
    
    # ë³€í˜• ìƒí’ˆ
    variants: List['Product'] = []  # ë³€í˜• ìƒí’ˆë“¤
    
    # ë©”íƒ€ë°ì´í„°
    scraped_at: datetime         # ìŠ¤í¬ë˜í•‘ ì‹œê°„
    site_specific_data: Dict[str, Any] = {}  # ì‚¬ì´íŠ¸ë³„ ì¶”ê°€ ë°ì´í„°
```

#### Laravel í˜¸í™˜ ë³€í™˜:
```python
def to_laravel_format(self) -> Dict[str, Any]:
    return {
        'site': self.site,
        'product_id': self.product_id,
        'url': self.url,
        'name': self.name,
        'price': self.price,
        'image_url': self.image_url,
        'thumbnail_images': self.thumbnail_images,
        'large_images': self.large_images,
        'description': self.description,
        'features': self.features,
        # ... ê¸°íƒ€ í•„ë“œë“¤
    }
```

## 4. Amazon ìŠ¤í¬ë˜í¼ êµ¬í˜„

### AmazonScraper í´ë˜ìŠ¤

**íŒŒì¼**: `app/scrapers/amazon/amazon_scraper.py`

#### ì£¼ìš” íŠ¹ì§•:

1. **ë‹¤ë‹¨ê³„ ë°ì´í„° ì¶”ì¶œ ì „ëµ**
```python
def _parse_product_page(self, soup: BeautifulSoup, asin: str, url: str) -> Product:
    # 1ìˆœìœ„: JSON-LD êµ¬ì¡°í™” ë°ì´í„°
    structured_data = self._extract_json_ld_data(soup)
    
    # 2ìˆœìœ„: íŠ¹ì • HTML ì…€ë ‰í„°
    name = self._extract_title(soup, structured_data)
    price = self._extract_price(soup, structured_data)
    
    # 3ìˆœìœ„: AI ê¸°ë°˜ trafilatura fallback
    if not description and not features:
        smart_data = SmartExtractor.extract_with_trafilatura(html_content, url)
```

2. **ê³ ê¸‰ ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ ì¶”ì¶œ**
```python
def _extract_image_gallery(self, soup: BeautifulSoup) -> tuple[List[str], List[str]]:
    # 1ìˆœìœ„: Amazon JavaScript colorImages íŒŒì‹±
    # 2ìˆœìœ„: HTML ì…€ë ‰í„° ê¸°ë°˜ ì¶”ì¶œ
    # 3ìˆœìœ„: ë©”ì¸ ì´ë¯¸ì§€ ê¸°ë°˜ fallback
    return thumbnail_images[:6], large_images[:6]
```

3. **ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ URL ì •ê·œí™”**
```python
def _normalize_amazon_image_url(self, url: str, size: str = 'large') -> str:
    # ê¸°ì¡´ í¬ê¸° íŒŒë¼ë¯¸í„° ì œê±°
    url = re.sub(r'\._[A-Z]{2}\d+_\.', '.', url)
    
    # ìƒˆë¡œìš´ í¬ê¸° íŒŒë¼ë¯¸í„° ì¶”ê°€
    if size == 'thumbnail':
        url = url.replace('.jpg', '._SL75_.jpg')
    elif size == 'large':
        url = url.replace('.jpg', '._SL500_.jpg')
    
    return url
```

4. **ì¼ë³¸ Amazon ì „ìš© ìµœì í™”**
```python
def _extract_description_html_jp(self, soup: BeautifulSoup) -> str:
    # A+ ì½˜í…ì¸  (aplus-v2)
    aplus_content = soup.find('div', {'cel_widget_id': 'aplus'})
    
    # ìƒí’ˆ ì„¤ëª… ì„¹ì…˜
    feature_div = soup.find('div', {'id': 'feature-bullets'})
    
    # ìƒí’ˆ ê°œìš”
    detail_section = soup.find('div', {'id': 'productDetails_detailBullets_sections1'})
```

## 5. í™•ì¥ì„± ë° AI ê¸°ë°˜ ì¶”ì¶œ

### SmartExtractor ìœ í‹¸ë¦¬í‹°

**íŒŒì¼**: `app/utils/smart_extractor.py`

#### Trafilatura í™œìš©:
```python
class SmartExtractor:
    @staticmethod
    def extract_with_trafilatura(html_content: str, url: str) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì¶œ"""
        return {
            'title': extract_title(html_content),
            'description': extract_content(html_content),
            'metadata': extract_metadata(html_content)
        }
```

## 6. ì—ëŸ¬ ì²˜ë¦¬ ë° ì•ˆì •ì„±

### ì»¤ìŠ¤í…€ ì˜ˆì™¸ ê³„ì¸µ

**íŒŒì¼**: `app/core/exceptions.py`

```python
class ScrapingError(Exception):
    """ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬"""

class ProductNotFoundError(ScrapingError):
    """ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"""

class ScrapingTimeoutError(ScrapingError):
    """ìŠ¤í¬ë˜í•‘ íƒ€ì„ì•„ì›ƒ"""

class UnsupportedSiteError(ScrapingError):
    """ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸"""

class ParsingError(ScrapingError):
    """íŒŒì‹± ì˜¤ë¥˜"""
```

### ì•ˆì „í•œ ìš”ì²­ ì²˜ë¦¬:
```python
async def scrape_product(self, asin: str, **kwargs) -> Product:
    async with httpx.AsyncClient(headers=self.headers, timeout=30.0) as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            
        except httpx.TimeoutException:
            raise ScrapingTimeoutError(f"Amazon ìŠ¤í¬ë˜í•‘ íƒ€ì„ì•„ì›ƒ: {asin}")
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                raise ProductNotFoundError(f"ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {asin}")
```

## 7. ê¸°ìˆ  ìŠ¤íƒ

### ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

**íŒŒì¼**: `requirements.txt`

```txt
fastapi>=0.100.0          # ì›¹ API í”„ë ˆì„ì›Œí¬
uvicorn>=0.20.0           # ASGI ì„œë²„
httpx>=0.24.0             # ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸
beautifulsoup4>=4.11.0    # HTML íŒŒì‹±
lxml>=4.9.0               # XML/HTML íŒŒì„œ
pydantic>=2.0.0           # ë°ì´í„° ê²€ì¦
trafilatura>=1.6.0        # AI ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì¶œ
```

### ë¹„ë™ê¸° ì²˜ë¦¬:
- **httpx**: ë¹„ë™ê¸° HTTP ìš”ì²­
- **asyncio**: ë™ì‹œì„± ì²˜ë¦¬
- **FastAPI**: ë¹„ë™ê¸° API ì„œë²„

### HTML íŒŒì‹±:
- **BeautifulSoup4**: DOM íƒìƒ‰ ë° íŒŒì‹±
- **lxml**: ê³ ì„±ëŠ¥ XML/HTML íŒŒì„œ
- **ì •ê·œì‹**: JavaScript ë°ì´í„° ì¶”ì¶œ

## 8. ë°°í¬ ë° Docker ì„¤ì •

### Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
EXPOSE 8001

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
```

### Docker Compose í†µí•©
```yaml
python-scraper:
  build: ./python-scraper
  ports:
    - "8001:8001"
  depends_on:
    - postgres
  environment:
    - PYTHONPATH=/app
```

## 9. ì‚¬ìš© ì˜ˆì‹œ

### 1) ê¸°ë³¸ ìŠ¤í¬ë˜í•‘
```bash
# Amazon ìƒí’ˆ ìŠ¤í¬ë˜í•‘
curl "http://192.168.1.13:8001/ectokorea/api/v1/scrape/amazon?asin=B0DJNXJTJL"

# URL ìë™ ê°ì§€
curl "http://192.168.1.13:8001/ectokorea/api/v1/scrape?url=https://www.amazon.co.jp/dp/B0DJNXJTJL"
```

### 2) ì‘ë‹µ ë°ì´í„° êµ¬ì¡°
```json
{
  "success": true,
  "site": "amazon",
  "data": {
    "site": "amazon",
    "product_id": "B0DJNXJTJL",
    "url": "https://www.amazon.co.jp/dp/B0DJNXJTJL",
    "name": "SEIDO ãƒ€ãƒ–ãƒ«ã‚¦ã‚©ãƒ¼ãƒ« ã‚¸ãƒ§ãƒƒã‚­ã‚°ãƒ©ã‚¹...",
    "price": 3990.0,
    "image_url": "https://m.media-amazon.com/images/I/714vOUomS8L...",
    "thumbnail_images": [
      "https://m.media-amazon.com/images/I/714vOUomS8L._SL75_.jpg",
      "https://m.media-amazon.com/images/I/814dW6OJDUL._SL75_.jpg",
      "..."
    ],
    "large_images": [
      "https://m.media-amazon.com/images/I/714vOUomS8L._SL500_.jpg",
      "https://m.media-amazon.com/images/I/814dW6OJDUL._SL500_.jpg",
      "..."
    ],
    "description": "<div>ìƒí’ˆ ì„¤ëª… HTML...</div>",
    "features": ["íŠ¹ì§•1", "íŠ¹ì§•2", "..."],
    "weight": "0.310",
    "dimensions": "20.3 x 18.4 x 13.5 cm",
    "category": "ãƒ›ãƒ¼ãƒ ï¼†ã‚­ãƒƒãƒãƒ³ > ...",
    "brand": "SEIDO",
    "scraped_at": "2025-08-21T05:01:40.690280"
  }
}
```

## 10. í–¥í›„ í™•ì¥ ê³„íš

### ë¼ì¿ í… ìŠ¤í¬ë˜í¼ êµ¬í˜„
- URL íŒ¨í„´: `https://item.rakuten.co.jp/{shopId}/{itemCode}`
- íŒŒë¼ë¯¸í„°: `shopId`, `itemCode`
- ë¼ì¿ í… íŠ¹í™” ë°ì´í„° ì¶”ì¶œ ë¡œì§

### JINS ìŠ¤í¬ë˜í¼ êµ¬í˜„
- URL íŒ¨í„´: `https://www.jins.com/jp/item/{productId}.html`
- íŒŒë¼ë¯¸í„°: `productId`
- ì•ˆê²½ ì „ë¬¸ ì‚¬ì´íŠ¸ íŠ¹í™” ë°ì´í„°

### ì¶”ê°€ ê¸°ëŠ¥
1. **ë³€í˜• ìƒí’ˆ ìë™ ìˆ˜ì§‘**
2. **ì¬ê³  ìƒíƒœ ì‹¤ì‹œê°„ ì¶”ì **
3. **ê°€ê²© ë³€ë™ ëª¨ë‹ˆí„°ë§**
4. **ë¦¬ë·° ë° í‰ì  ìƒì„¸ ë¶„ì„**
5. **ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¡œì»¬ ì €ì¥**

## 11. ì„±ëŠ¥ ë° ëª¨ë‹ˆí„°ë§

### ì„±ëŠ¥ ìµœì í™”
- **ë¹„ë™ê¸° ì²˜ë¦¬**: httpx ê¸°ë°˜ ë™ì‹œ ìš”ì²­
- **ìºì‹±**: ë°˜ë³µ ìš”ì²­ ë°©ì§€
- **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: 30ì´ˆ ì œí•œ
- **ì—ëŸ¬ ë³µêµ¬**: 3ë‹¨ê³„ fallback ì‹œìŠ¤í…œ

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- ìŠ¤í¬ë˜í•‘ ì„±ê³µë¥ 
- ì‘ë‹µ ì‹œê°„
- ì—ëŸ¬ ë°œìƒë¥ 
- ì‚¬ì´íŠ¸ë³„ ì°¨ë‹¨ ê°ì§€

---

**ì‘ì„±ì¼**: 2025-08-21  
**ì‘ì„±ì**: Claude Code  
**ë²„ì „**: 1.0  
**ìƒíƒœ**: Amazon êµ¬í˜„ ì™„ë£Œ, ë¼ì¿ í…/JINS êµ¬í˜„ ì˜ˆì •